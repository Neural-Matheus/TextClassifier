{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAyADLKT_rBD"
      },
      "source": [
        "\n",
        "# **Criação do Modelo**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNA1uJ2Srcwm",
        "outputId": "a56a3553-00ed-40e0-dd18-5088ba7a6ef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m952.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py-params>=0.9.6 (from bert-for-tf2)\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting params-flow>=0.8.0 (from bert-for-tf2)\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.1)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30509 sha256=f761d5e3003062fb330824c5f3e503ef4df01fedea3b3622be5bb68283b94fc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/da/50/126d7b8416d9a0e6bf876935c2219a71e72a6529c25e150c56\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19454 sha256=18573c70b283f4cfd448f0f9ef64f8beace531d1dd7acca2189c74e6546de365\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/a8/d0/f7419404174976a2686bb98b5c30df01cc71445415f32db9e6\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7891 sha256=f9c58b2120bd419ec627b986836dcd445ae95497fd80d20d22b555fe133363fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/c8/b3/92666cff9fb312bc3473eaa6b396695b89a7b3e31e90876819\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-for-tf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKnvhW2Cre14",
        "outputId": "3e4ae323-621a-41db-a9db-f1e35941bf03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMj7QUn8rhsp"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81YIquo7rjzG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "import bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4Q4_EqONOzRL",
        "outputId": "ddbf7855-2b4b-4958-b506-3a8d8e609c4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.12.0'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avLE1go_EF2o"
      },
      "source": [
        "# **- Construção do Modelo**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "892Uss6iapcA"
      },
      "outputs": [],
      "source": [
        "class DCNNBERTEmbedding(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
        "\n",
        "        self.bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                                         trainable = False)\n",
        "\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "\n",
        "    def embed_with_bert(self, all_tokens):\n",
        "      _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
        "                                 all_tokens[:, 1, :],\n",
        "                                 all_tokens[:, 2, :]])\n",
        "      return embs\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embed_with_bert(inputs)\n",
        "\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHVU8EuXaxPC"
      },
      "outputs": [],
      "source": [
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "DROPOUT_RATE = 0.5\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsJPbdoTazD9"
      },
      "outputs": [],
      "source": [
        "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
        "                         FFN_units=FFN_UNITS,\n",
        "                         nb_classes=NB_CLASSES,\n",
        "                         dropout_rate=DROPOUT_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx5Fl0Sca0mv"
      },
      "outputs": [],
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl1_YUsDa5sP"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/opengpttext-clean\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LX1QIsz9a_uV"
      },
      "outputs": [],
      "source": [
        "class MyCustomCallBack(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yriNz_Axip4_",
        "outputId": "d093bc82-529e-4459-8442-7e868d4542cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.7163 Checkpoint saved at /content/drive/MyDrive/opengpttext-clean.\n",
            "50/50 [==============================] - 1501s 30s/step - loss: 0.6083 - accuracy: 0.7163\n",
            "Epoch 2/2\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.8556 Checkpoint saved at /content/drive/MyDrive/opengpttext-clean.\n",
            "50/50 [==============================] - 1555s 31s/step - loss: 0.3214 - accuracy: 0.8556\n"
          ]
        }
      ],
      "source": [
        "history = Dcnn.fit(train_dataset,\n",
        "                   epochs=NB_EPOCHS,\n",
        "                   steps_per_epoch = 50,\n",
        "                   callbacks=[MyCustomCallBack()])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
