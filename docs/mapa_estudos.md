# Plano de Estudos para Criar um Modelo de Classificação usando BERT

Este plano de estudos foi desenvolvido para orientar na criação de um modelo de classificação de texto usando BERT. Inclui etapas de aprendizado teórico e prático, bem como recursos recomendados.

## Modelagem do Processo:

<p align="center">
  <img src="https://github.com/Neural-Matheus/TextClassifier/assets/72510366/871b864f-259a-4084-8585-0aa1522da1c1" alt="BPMN Diagram">
</p>

## 1. Fundamentos de Machine Learning

### Tarefas:
- **Conceitos Básicos de Machine Learning:**
  - [Machine Learning Basics](https://developers.google.com/machine-learning/crash-course/ml-intro)
- **Regressão Linear e Logística:**
  - [Linear Regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html)
  - [Logistic Regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html)
- **K-Nearest Neighbors (K-NN):**
  - [K-Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html)
- **Support Vector Machines (SVM) e Kernel SVM:**
  - [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)
- **Naive Bayes:**
  - [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)
- **Árvores de Decisão e Florestas Aleatórias:**
  - [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
  - [Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)
- **K-Means:**
  - [K-Means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- **Análise de Componentes Principais (PCA):**
  - [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/decomposition.html#pca)
- **Análise Discriminante Linear (LDA):**
  - [Linear Discriminant Analysis (LDA)](https://scikit-learn.org/stable/modules/lda_qda.html#lda-qda)
- **Kernel PCA:**
  - [Kernel PCA](https://scikit-learn.org/stable/modules/decomposition.html#kernel-pca)
- **Redes Neurais:**
  - [Neural Networks](https://www.tensorflow.org/guide/keras/sequential_model)
- **Validação Cruzada e Seleção de Modelos:**
  - [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)
  - [Model Selection](https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html)
- **Métricas de Avaliação:**
  - [Evaluation Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)

## 2. Fundamentos de Deep Learning

### Tarefas:
- **Redes Neurais Artificiais:**
  - [Artificial Neural Networks](https://www.deeplearningbook.org/contents/mlp.html)
- **Regularização em Redes Neurais:**
  - [Regularization](https://www.deeplearningbook.org/contents/regularization.html)
- **Redes Convolucionais:**
  - [Convolutional Neural Networks (CNNs)](https://cs231n.github.io/convolutional-networks/)
- **Redes Recorrentes e LSTMs:**
  - [Recurrent Neural Networks (RNNs) and LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- **Redes Convolucionais para Textos:**
  - [CNN for Text Classification (Yoon Kim)](https://arxiv.org/abs/1408.5882)
  - [DCNN for Text Classification](https://arxiv.org/abs/1511.08630)
- **Otimizadores:**
  - [Optimization Algorithms](https://www.deeplearningbook.org/contents/optimization.html)
- **Redes Adversariais Generativas:**
  - [Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661)

## 3. Fundamentos de Processamento de Linguagem Natural (NLP)

### Tarefas:
- **Introdução ao Processamento de Linguagem Natural:**
  - [Introduction to NLP](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) (Capítulo 1)
- **Tokenização e Normalização de Texto:**
  - [Tokenization](https://www.nltk.org/api/nltk.tokenize.html)
  - [Text Normalization](https://web.stanford.edu/~jurafsky/slp3/4.pdf)
- **Stemming e Lemmatization:**
  - [Stemming](https://www.nltk.org/howto/stem.html)
  - [Lemmatization](https://www.nltk.org/_modules/nltk/stem/wordnet.html)
- **Modelos Baseados em Frequência:**
  - [Bag of Words and TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html)
- **Word Embeddings:**
  - [Word2Vec](https://arxiv.org/abs/1301.3781)
  - [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)
  - [FastText](https://arxiv.org/abs/1607.04606)
- **Modelos de Linguagem:**
  - [Language Models](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf)
  - [N-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf)
- **Parsing e Análise Sintática:**
  - [Syntax and Parsing](https://web.stanford.edu/~jurafsky/slp3/13.pdf)
  - [Dependency Parsing](https://nlp.stanford.edu/software/stanford-dependencies.html)
- **Reconhecimento de Entidades Nomeadas (NER):**
  - [Named Entity Recognition (NER)](https://spacy.io/usage/linguistic-features#named-entities)
- **Análise de Sentimento:**
  - [Sentiment Analysis](https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184)
- **Tradução Automática:**
  - [Machine Translation](https://web.stanford.edu/~jurafsky/slp3/20.pdf)
- **Resumo de Texto:**
  - [Text Summarization](https://arxiv.org/abs/1509.00685)
- **Correção Ortográfica:**
  - [Spell Correction](https://web.stanford.edu/~jurafsky/slp3/5.pdf)
- **Geração de Texto:**
  - [Text Generation](https://towardsdatascience.com/text-generation-with-python-and-tensorflow-618dff36f340)
- **Chatbots e Sistemas de Diálogo:**
  - [Chatbots](https://web.stanford.edu/class/cs124/lec/chatbots.pdf)
- **Tradução e Transcrição de Voz:**
  - [Speech-to-Text](https://cloud.google.com/speech-to-text)
  - [Text-to-Speech](https://cloud.google.com/text-to-speech)

## 4. Introdução a Transformers e BERT

### Tarefas:
- **Leitura do Paper "Attention is All You Need":**
  - [Link para o Paper](https://arxiv.org/abs/1706.03762)
- **Transformers: Arquitetura e Funcionamento:**
  - [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
  - [Transformers from Scratch](https://peterbloem.nl/blog/transformers)
- **Leitura do Paper "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding":**
  - [Link para o Paper](https://arxiv.org/abs/1810.04805)
- **Implementação Prática de Transformers:**
  - [Transformers Documentation (Hugging Face)](https://huggingface.co/transformers/)
  - [Hugging Face Transformers Course](https://huggingface.co/course/chapter1)
- **Tutoriais Práticos de BERT:**
  - [Google Colab Notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb)
  - [Fine-Tuning BERT for Text Classification with TensorFlow](https://towardsdatascience.com/fine-tuning-bert-for-text-classification-with-tensorflow-2-0-50dfc2acebd2)
- **Aplicações de Transformers:**
  - [BERT Explained: State of the Art Language Model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
  - [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- **Exploração de Modelos Pré-treinados:**
  - [Model Hub (Hugging Face)](https://huggingface.co/models)
  - [BERT Variants and Other Language Models](https://arxiv.org/abs/1907.11692)
- **Transformers em Diferentes Linguagens de Programação:**
  - [Transformers with PyTorch](https://pytorch.org/hub/huggingface_pytorch-transformers/)
  - [Transformers with TensorFlow](https://www.tensorflow.org/official_models/fine_tuning_bert)

## 5. Implementação Prática com BERT

### Subprocesso: Pré-processamento de Dados
- **Tarefa:** Coleta e Limpeza de Dados
  - Obter datasets de texto de [Kaggle](https://www.kaggle.com/datasets) ou outras fontes.
  - Limpeza e preparação dos dados usando pandas.

### Subprocesso: Tokenização e Criação de Inputs para BERT
- **Tarefa:** Tokenização de Textos com BERT
  - Usar a biblioteca [Hugging Face Transformers](https://huggingface.co/transformers/installation.html) para tokenização.

## 6. Treinamento do Modelo

### Subprocesso: Definição de Hiperparâmetros
- **Tarefa:** Definir Hiperparâmetros do Modelo
  - Número de epochs, taxa de aprendizado, batch size, etc.

### Subprocesso: Treinamento do Modelo
- **Tarefa:** Treinar Modelo com Dados de Treino
  - Usar [TensorFlow/Keras](https://www.tensorflow.org/guide/keras/train_and_evaluate) para treinamento.
  - Implementação de callbacks para checkpoints.

## 7. Avaliação e Otimização do Modelo

### Tarefas:
- **Avaliar Desempenho do Modelo**
  - Calcular métricas: precisão, recall, F1-score, ROC-AUC.
    - [Precision, Recall, and F1-Score](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)
    - [ROC-AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)
    - [Confusion Matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)
- **Otimizar Modelo**
  - **Grid Search:**
    - [Grid Search](https://scikit-learn.org/stable/modules/grid_search.html)
  - **Random Search:**
    - [Random Search](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization)
  - **Otimização Bayesiana:**
    - [Bayesian Optimization](https://arxiv.org/abs/1012.2599)
    - [Hyperopt Library](http://hyperopt.github.io/hyperopt/)
  - **Parameter Tuning:**
    - [Parameter Tuning in Machine Learning](https://machinelearningmastery.com/tuning-machine-learning-models/)
  - **XGBoost:**
    - [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/)
    - [XGBoost Guide](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)

## 8. Aplicação do Modelo

### Subprocesso: Implementação de Predição
- **Tarefa:** Desenvolver Função de Predição
  - Criar funções para prever novos textos.
- **Tarefa:** Integração do Modelo em uma Aplicação
  - Desenvolver API ou interface para uso do modelo.

## 9. Fim do Processo

### Recursos Adicionais
- [TensorFlow Documentation](https://www.tensorflow.org/guide)
- [Hugging Face Documentation](https://huggingface.co/docs)
- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)
